{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "184482b1",
      "metadata": {
        "papermill": {
          "duration": 0.010475,
          "end_time": "2023-04-25T20:22:32.109773",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.099298",
          "status": "completed"
        },
        "tags": [],
        "id": "184482b1"
      },
      "source": [
        "<h1><center><b>ChemistrAI</b></center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3318d2b",
      "metadata": {
        "papermill": {
          "duration": 0.008645,
          "end_time": "2023-04-25T20:22:32.127516",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.118871",
          "status": "completed"
        },
        "tags": [],
        "id": "f3318d2b"
      },
      "source": [
        "# **Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178b8fbd",
      "metadata": {
        "papermill": {
          "duration": 0.008272,
          "end_time": "2023-04-25T20:22:32.144290",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.136018",
          "status": "completed"
        },
        "tags": [],
        "id": "178b8fbd"
      },
      "source": [
        "I will train this model deterministically using standard backpropagation techniques and stochastically by maximizing a variational minor tied. \n",
        "I will also show through visualization how the model is capable of automatically learning to fix its look at highlighted objects while generating the corresponding words in the output stream"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84eb12f6",
      "metadata": {
        "papermill": {
          "duration": 0.008363,
          "end_time": "2023-04-25T20:22:32.161396",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.153033",
          "status": "completed"
        },
        "tags": [],
        "id": "84eb12f6"
      },
      "source": [
        "It is a combination of convolutional neural networks to obtain the vector representation of images and recurrent neural networks to decode those representations into natural language sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbdd6c7",
      "metadata": {
        "papermill": {
          "duration": 0.008327,
          "end_time": "2023-04-25T20:22:32.178241",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.169914",
          "status": "completed"
        },
        "tags": [],
        "id": "cdbdd6c7"
      },
      "source": [
        "![arquitectura de modelo gen ai.png](attachment:0c22d97b-9d8e-4931-88f8-b5d601742851.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b86ff7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:32.197439Z",
          "iopub.status.busy": "2023-04-25T20:22:32.196983Z",
          "iopub.status.idle": "2023-04-25T20:22:38.105946Z",
          "shell.execute_reply": "2023-04-25T20:22:38.104672Z"
        },
        "papermill": {
          "duration": 5.922104,
          "end_time": "2023-04-25T20:22:38.108846",
          "exception": false,
          "start_time": "2023-04-25T20:22:32.186742",
          "status": "completed"
        },
        "tags": [],
        "id": "70b86ff7"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "    \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import sys\n",
        "sys.path.append('/timm-pytorch-image-models/pytorch-image-models-master')\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import Levenshtein #Metric in training\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "import torchvision.models as models\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "from albumentations import (\n",
        "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
        "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
        "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
        "    )\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import ImageOnlyTransform\n",
        "\n",
        "import timm\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10a125c",
      "metadata": {
        "papermill": {
          "duration": 0.00866,
          "end_time": "2023-04-25T20:22:38.126188",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.117528",
          "status": "completed"
        },
        "tags": [],
        "id": "d10a125c"
      },
      "source": [
        "# **EfficientNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baec06ae",
      "metadata": {
        "papermill": {
          "duration": 0.009023,
          "end_time": "2023-04-25T20:22:38.143921",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.134898",
          "status": "completed"
        },
        "tags": [],
        "id": "baec06ae"
      },
      "source": [
        "EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. \n",
        "\n",
        "For example, if we want to use 2ⁿ times more computational resources, then we can simply increase the network depth by αⁿ, width by βⁿ, and image size by γⁿ, where α,β,γ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient ϕ to uniformly scales network width, depth, and resolution in a principled way.\n",
        "\n",
        "The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785019b8",
      "metadata": {
        "papermill": {
          "duration": 0.008329,
          "end_time": "2023-04-25T20:22:38.160985",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.152656",
          "status": "completed"
        },
        "tags": [],
        "id": "785019b8"
      },
      "source": [
        "![efficientnet_architecture.png](attachment:459f2701-3bd5-4287-b4ce-b2aea464c2f9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6003be",
      "metadata": {
        "papermill": {
          "duration": 0.00818,
          "end_time": "2023-04-25T20:22:38.177879",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.169699",
          "status": "completed"
        },
        "tags": [],
        "id": "bd6003be"
      },
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a4453a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.197484Z",
          "iopub.status.busy": "2023-04-25T20:22:38.196418Z",
          "iopub.status.idle": "2023-04-25T20:22:38.204225Z",
          "shell.execute_reply": "2023-04-25T20:22:38.203295Z"
        },
        "papermill": {
          "duration": 0.020106,
          "end_time": "2023-04-25T20:22:38.206512",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.186406",
          "status": "completed"
        },
        "tags": [],
        "id": "d8a4453a"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    debug          = True\n",
        "    apex           = False\n",
        "    max_len        = 275\n",
        "    print_freq     = 250\n",
        "    num_workers    = 4\n",
        "    model_name     = 'efficientnet_b2'\n",
        "    enc_size       = 1408\n",
        "    samp_size      = 100000\n",
        "    size           = 288\n",
        "    scheduler      = 'CosineAnnealingLR' \n",
        "    epochs         = 1 \n",
        "    T_max          = 4  \n",
        "    encoder_lr     = 1e-4\n",
        "    decoder_lr     = 4e-4\n",
        "    min_lr         = 1e-6\n",
        "    batch_size     = 32\n",
        "    weight_decay   = 1e-6\n",
        "    gradient_accumulation_steps = 1\n",
        "    max_grad_norm  = 10\n",
        "    attention_dim  = 256\n",
        "    embed_dim      = 512\n",
        "    decoder_dim    = 512\n",
        "    decoder_layers = 2     # number of LSTM layers\n",
        "    dropout        = 0.5\n",
        "    seed           = 42\n",
        "    n_fold         = 5\n",
        "    trn_fold       = 0 \n",
        "    train          = True\n",
        "    train_path     = '/wowdao-generative-ai-applications-dataset/data/train/'\n",
        "    prep_path      = '/tokenizer-and-preprocessing-of-chemical-formulas/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16556cb",
      "metadata": {
        "papermill": {
          "duration": 0.008147,
          "end_time": "2023-04-25T20:22:38.223313",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.215166",
          "status": "completed"
        },
        "tags": [],
        "id": "e16556cb"
      },
      "source": [
        "# **Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db45aa94",
      "metadata": {
        "papermill": {
          "duration": 0.008141,
          "end_time": "2023-04-25T20:22:38.240042",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.231901",
          "status": "completed"
        },
        "tags": [],
        "id": "db45aa94"
      },
      "source": [
        "The tokenizer developed in the previous part of this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "468a74a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.259091Z",
          "iopub.status.busy": "2023-04-25T20:22:38.258329Z",
          "iopub.status.idle": "2023-04-25T20:22:38.281081Z",
          "shell.execute_reply": "2023-04-25T20:22:38.279937Z"
        },
        "papermill": {
          "duration": 0.035515,
          "end_time": "2023-04-25T20:22:38.284180",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.248665",
          "status": "completed"
        },
        "tags": [],
        "id": "468a74a4",
        "outputId": "a080fee0-9dda-4cd5-a07d-cbda45e1261c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n"
          ]
        }
      ],
      "source": [
        "class Tokenizer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)\n",
        "    \n",
        "    def fit_on_texts(self, texts):\n",
        "        vocab = set()\n",
        "        for text in texts:\n",
        "            vocab.update(text.split(' '))\n",
        "        vocab = sorted(vocab)\n",
        "        vocab.append('<sos>')\n",
        "        vocab.append('<eos>')\n",
        "        vocab.append('<pad>')\n",
        "        for i, s in enumerate(vocab):\n",
        "            self.stoi[s] = i\n",
        "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
        "        \n",
        "    def text_to_sequence(self, text):\n",
        "        sequence = []\n",
        "        sequence.append(self.stoi['<sos>'])\n",
        "        for s in text.split(' '):\n",
        "            sequence.append(self.stoi[s])\n",
        "        sequence.append(self.stoi['<eos>'])\n",
        "        return sequence\n",
        "    \n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            sequence = self.text_to_sequence(text)\n",
        "            sequences.append(sequence)\n",
        "        return sequences\n",
        "\n",
        "    def sequence_to_text(self, sequence):\n",
        "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
        "    \n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for sequence in sequences:\n",
        "            text = self.sequence_to_text(sequence)\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "    \n",
        "    def predict_caption(self, sequence):\n",
        "        caption = ''\n",
        "        for i in sequence:\n",
        "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
        "                break\n",
        "            caption += self.itos[i]\n",
        "        return caption\n",
        "    \n",
        "    def predict_captions(self, sequences):\n",
        "        captions = []\n",
        "        for sequence in sequences:\n",
        "            caption = self.predict_caption(sequence)\n",
        "            captions.append(caption)\n",
        "        return captions\n",
        "\n",
        "tokenizer = torch.load(CFG.prep_path + 'tokenizer.pth')\n",
        "print(f\"tokenizer.stoi: {tokenizer.stoi}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a74c56",
      "metadata": {
        "papermill": {
          "duration": 0.008368,
          "end_time": "2023-04-25T20:22:38.301442",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.293074",
          "status": "completed"
        },
        "tags": [],
        "id": "65a74c56"
      },
      "source": [
        "# **Metric for evaluation (Levenshtein distance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cedd6c5",
      "metadata": {
        "papermill": {
          "duration": 0.008248,
          "end_time": "2023-04-25T20:22:38.318477",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.310229",
          "status": "completed"
        },
        "tags": [],
        "id": "2cedd6c5"
      },
      "source": [
        "It is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. \n",
        "\n",
        "Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics known collectively as edit distance.It is closely related to pairwise string alignments.\n",
        "\n",
        "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following 3 edits change one into the other, and there is no way to do it with fewer than 3 edits:\n",
        "\n",
        "* kitten → sitten (substitution of \"s\" for \"k\"),\n",
        "* sitten → sittin (substitution of \"i\" for \"e\"),\n",
        "* sittin → sitting (insertion of \"g\" at the end)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946fa8c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.337227Z",
          "iopub.status.busy": "2023-04-25T20:22:38.336911Z",
          "iopub.status.idle": "2023-04-25T20:22:38.348810Z",
          "shell.execute_reply": "2023-04-25T20:22:38.347892Z"
        },
        "papermill": {
          "duration": 0.023649,
          "end_time": "2023-04-25T20:22:38.350847",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.327198",
          "status": "completed"
        },
        "tags": [],
        "id": "946fa8c3"
      },
      "outputs": [],
      "source": [
        "#Metric in training\n",
        "def get_score(y_true, y_pred):\n",
        "    scores = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        score = Levenshtein.distance(true, pred)\n",
        "        scores.append(score)\n",
        "    avg_score = np.mean(scores)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "#Fix the random seed\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed = CFG.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e141f9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.370268Z",
          "iopub.status.busy": "2023-04-25T20:22:38.369404Z",
          "iopub.status.idle": "2023-04-25T20:22:38.380562Z",
          "shell.execute_reply": "2023-04-25T20:22:38.379607Z"
        },
        "papermill": {
          "duration": 0.023217,
          "end_time": "2023-04-25T20:22:38.382567",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.359350",
          "status": "completed"
        },
        "tags": [],
        "id": "27e141f9"
      },
      "outputs": [],
      "source": [
        "# Datasets transformers to convert the image files to tensors (arrays) and tokenize the InChI texts\n",
        "\n",
        "# The training dataset will contain \n",
        "# * the image (array)\n",
        "# * tokenized  InChI texts\n",
        "# * lengh of tokenized InChI texts\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform=None):\n",
        "        super().__init__()\n",
        "        self.df         = df\n",
        "        self.tokenizer  = tokenizer\n",
        "        self.file_paths = df['file_path'].values\n",
        "        self.labels     = df['InChI_text'].values\n",
        "        self.transform  = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        image = cv2.imread(file_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image = image)\n",
        "            image     = augmented['image']\n",
        "        label = self.labels[idx]\n",
        "        label = self.tokenizer.text_to_sequence(label)\n",
        "        label_length = len(label)\n",
        "        label_length = torch.LongTensor([label_length])\n",
        "        return image, torch.LongTensor(label), label_length\n",
        "    \n",
        "# The testing dataset will contain \n",
        "# * the image (array)\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.file_paths = df['file_path'].values\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        image = cv2.imread(file_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f33658",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.402185Z",
          "iopub.status.busy": "2023-04-25T20:22:38.401635Z",
          "iopub.status.idle": "2023-04-25T20:22:38.407727Z",
          "shell.execute_reply": "2023-04-25T20:22:38.406611Z"
        },
        "papermill": {
          "duration": 0.018257,
          "end_time": "2023-04-25T20:22:38.409869",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.391612",
          "status": "completed"
        },
        "tags": [],
        "id": "34f33658"
      },
      "outputs": [],
      "source": [
        "#Concatenates a sequence of tensors along a new dimension\n",
        "#All tensors will be of the same size\n",
        "def bms_collate(batch):\n",
        "    imgs, labels, label_lengths = [], [], []\n",
        "    for data_point in batch:\n",
        "        imgs.append(data_point[0])\n",
        "        labels.append(data_point[1])\n",
        "        label_lengths.append(data_point[2])\n",
        "    labels = pad_sequence(labels, batch_first = True, padding_value = tokenizer.stoi[\"<pad>\"])\n",
        "    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836f2992",
      "metadata": {
        "papermill": {
          "duration": 0.008459,
          "end_time": "2023-04-25T20:22:38.426708",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.418249",
          "status": "completed"
        },
        "tags": [],
        "id": "836f2992"
      },
      "source": [
        "# **CNN Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797a2b94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.445242Z",
          "iopub.status.busy": "2023-04-25T20:22:38.444945Z",
          "iopub.status.idle": "2023-04-25T20:22:38.450970Z",
          "shell.execute_reply": "2023-04-25T20:22:38.449942Z"
        },
        "papermill": {
          "duration": 0.017853,
          "end_time": "2023-04-25T20:22:38.453029",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.435176",
          "status": "completed"
        },
        "tags": [],
        "id": "797a2b94"
      },
      "outputs": [],
      "source": [
        "#CNN Encoder\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, model_name = CFG.model_name, pretrained = False):\n",
        "        super().__init__()\n",
        "        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs       = x.size(0)\n",
        "        features = self.cnn.forward_features(x)\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70cc557",
      "metadata": {
        "papermill": {
          "duration": 0.008561,
          "end_time": "2023-04-25T20:22:38.470044",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.461483",
          "status": "completed"
        },
        "tags": [],
        "id": "a70cc557"
      },
      "source": [
        "# **RNN Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fcfb66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.489134Z",
          "iopub.status.busy": "2023-04-25T20:22:38.488851Z",
          "iopub.status.idle": "2023-04-25T20:22:38.526567Z",
          "shell.execute_reply": "2023-04-25T20:22:38.525474Z"
        },
        "papermill": {
          "duration": 0.050147,
          "end_time": "2023-04-25T20:22:38.528652",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.478505",
          "status": "completed"
        },
        "tags": [],
        "id": "16fcfb66"
      },
      "outputs": [],
      "source": [
        "#RNN Decoder\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention network for calculate attention value\n",
        "    '''\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        '''\n",
        "        :param encoder_dim: input size of encoder network\n",
        "        :param decoder_dim: input size of decoder network\n",
        "        :param attention_dim: input size of attention network\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n",
        "        self.relu        = nn.ReLU()\n",
        "        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n",
        "        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n",
        "        return attention_weighted_encoding, alpha\n",
        "    \n",
        "    \n",
        "#Custom LSTM cell\n",
        "def LSTMCell(input_size, hidden_size, **kwargs):\n",
        "    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name or 'bias' in name:\n",
        "            param.data.uniform_(-0.1, 0.1)\n",
        "    return m\n",
        "\n",
        "# Decoder\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    '''\n",
        "    Decoder network with attention network used for training\n",
        "    '''\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n",
        "        '''\n",
        "        :param attention_dim: input size of attention network\n",
        "        :param embed_dim: input size of embedding network\n",
        "        :param decoder_dim: input size of decoder network\n",
        "        :param vocab_size: total number of characters used in training\n",
        "        :param encoder_dim: input size of encoder network\n",
        "        :param num_layers: number of the LSTM layers\n",
        "        :param dropout: dropout rate\n",
        "        '''\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "        self.encoder_dim   = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim     = embed_dim\n",
        "        self.decoder_dim   = decoder_dim\n",
        "        self.vocab_size    = vocab_size\n",
        "        self.dropout       = dropout\n",
        "        self.num_layers    = num_layers\n",
        "        self.device        = device\n",
        "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n",
        "        self.dropout       = nn.Dropout(p = self.dropout)\n",
        "        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n",
        "        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid       = nn.Sigmoid()\n",
        "        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()                                      # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune = True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim = 1)\n",
        "        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n",
        "        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        '''\n",
        "        :param encoder_out: output of encoder network\n",
        "        :param encoded_captions: transformed sequence from character to integer\n",
        "        :param caption_lengths: length of transformed sequence\n",
        "        '''\n",
        "        batch_size       = encoder_out.size(0)\n",
        "        encoder_dim      = encoder_out.size(-1)\n",
        "        vocab_size       = self.vocab_size\n",
        "        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels       = encoder_out.size(1)\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n",
        "        encoder_out      = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        \n",
        "        # embedding transformed sequence for vector\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "        \n",
        "        # Initialize LSTM state, initialize cell_vector and hidden_vector\n",
        "        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "        \n",
        "        # set decode length by caption length - 1 because of omitting start token\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n",
        "        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n",
        "        \n",
        "        # predict sequence\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                prev_h[-1][:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "\n",
        "            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
        "            \n",
        "            for i, rnn in enumerate(self.decode_step):\n",
        "                # recurrent cell\n",
        "                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n",
        "\n",
        "                # hidden state becomes the input to the next layer\n",
        "                input = self.dropout(h)\n",
        "\n",
        "                # save state for next time step\n",
        "                prev_h[i] = h\n",
        "                prev_c[i] = c\n",
        "                \n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :]      = alpha\n",
        "            \n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
        "    \n",
        "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
        "        \n",
        "        # size variables\n",
        "        batch_size  = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size  = self.vocab_size\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels  = encoder_out.size(1)\n",
        "        \n",
        "        # embed start tocken for LSTM input\n",
        "        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n",
        "        embeddings    = self.embedding(start_tockens)\n",
        "        \n",
        "        # initialize hidden state and cell state of LSTM cell\n",
        "        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n",
        "        \n",
        "        # predict sequence\n",
        "        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n",
        "        for t in range(decode_lengths):\n",
        "            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
        "            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
        "            awe        = gate * awe\n",
        "            \n",
        "            input = torch.cat([embeddings, awe], dim=1)\n",
        " \n",
        "            for j, rnn in enumerate(self.decode_step):\n",
        "                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
        "                input = self.dropout(at_h)\n",
        "                h[j]  = at_h\n",
        "                c[j]  = at_c\n",
        "            \n",
        "            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
        "            predictions[:, t, :] = preds\n",
        "            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n",
        "            if end_condition.sum() == batch_size:\n",
        "                break\n",
        "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
        "        \n",
        "        return predictions\n",
        "    \n",
        "    # beam search\n",
        "    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n",
        "        \n",
        "        h, c = hidden\n",
        "        #h, c = h.squeeze(0), c.squeeze(0)\n",
        "        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n",
        "        \n",
        "        embeddings = self.embedding(prev_tokens)\n",
        "        if embeddings.dim() == 3:\n",
        "            embeddings = embeddings.squeeze(1)\n",
        "            \n",
        "        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
        "        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
        "        awe        = gate * awe\n",
        "        \n",
        "        input = torch.cat([embeddings, awe], dim = 1)\n",
        "        for j, rnn in enumerate(self.decode_step):\n",
        "            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
        "            input = self.dropout(at_h)\n",
        "            h[j]  = at_h\n",
        "            c[j]  = at_c\n",
        "\n",
        "        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
        "\n",
        "        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n",
        "        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n",
        "        predicted_softmax = function(preds, dim = 1)\n",
        "        \n",
        "        return predicted_softmax, hidden, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ffe908",
      "metadata": {
        "papermill": {
          "duration": 0.009091,
          "end_time": "2023-04-25T20:22:38.546784",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.537693",
          "status": "completed"
        },
        "tags": [],
        "id": "a3ffe908"
      },
      "source": [
        "# **Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a60443",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.566037Z",
          "iopub.status.busy": "2023-04-25T20:22:38.565658Z",
          "iopub.status.idle": "2023-04-25T20:22:38.586531Z",
          "shell.execute_reply": "2023-04-25T20:22:38.585401Z"
        },
        "papermill": {
          "duration": 0.033287,
          "end_time": "2023-04-25T20:22:38.588697",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.555410",
          "status": "completed"
        },
        "tags": [],
        "id": "37a60443"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val   = 0\n",
        "        self.avg   = 0\n",
        "        self.sum   = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val    = val\n",
        "        self.sum   += val * n\n",
        "        self.count += n\n",
        "        self.avg    = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s   = now - since\n",
        "    es  = s / (percent)\n",
        "    rs  = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def train_fn(train_loader, encoder, decoder, criterion,encoder_optimizer, decoder_optimizer,epoch,encoder_scheduler, decoder_scheduler, device):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    losses     = AverageMeter()\n",
        "    # Switch to train mode\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    \n",
        "    for step, (images, labels, label_lengths) in enumerate(train_loader):    \n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images        = images.to(device)\n",
        "        labels        = labels.to(device)\n",
        "        label_lengths = label_lengths.to(device)\n",
        "        batch_size    = images.size(0)\n",
        "    \n",
        "        features = encoder(images)\n",
        "        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n",
        "        targets     = caps_sorted[:, 1:]\n",
        "        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n",
        "        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
        "        loss        = criterion(predictions, targets)\n",
        "        \n",
        "        # Record loss\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        if CFG.apex:\n",
        "            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n",
        "        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n",
        "                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n",
        "                  #'Encoder LR: {encoder_lr:.6f}  '\n",
        "                  #'Decoder LR: {decoder_lr:.6f}  '\n",
        "                  .format(\n",
        "                   epoch+1, step, len(train_loader), \n",
        "                   batch_time        = batch_time,\n",
        "                   data_time         = data_time, \n",
        "                   loss              = losses,\n",
        "                   remain            = timeSince(start, float(step+1)/len(train_loader)),\n",
        "                   encoder_grad_norm = encoder_grad_norm,\n",
        "                   decoder_grad_norm = decoder_grad_norm,\n",
        "                   #encoder_lr=encoder_scheduler.get_lr()[0],\n",
        "                   #decoder_lr=decoder_scheduler.get_lr()[0],\n",
        "                   ))\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    # Switch to evaluation mode\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    text_preds = []\n",
        "    start = end = time.time()\n",
        "    for step, (images) in enumerate(valid_loader):\n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        images     = images.to(device)\n",
        "        batch_size = images.size(0)\n",
        "        with torch.no_grad():\n",
        "            features    = encoder(images)\n",
        "            predictions = decoder.predict(features, CFG.max_len, tokenizer) \n",
        "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
        "        _text_preds = tokenizer.predict_captions(predicted_sequence)\n",
        "        text_preds.append(_text_preds)\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  .format(\n",
        "                      step, \n",
        "                      len(valid_loader),\n",
        "                      batch_time = batch_time,\n",
        "                      data_time = data_time,\n",
        "                      remain = timeSince(start, float(step+1)/len(valid_loader)),\n",
        "                  ))\n",
        "    text_preds = np.concatenate(text_preds)\n",
        "    return text_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb2b357",
      "metadata": {
        "papermill": {
          "duration": 0.008338,
          "end_time": "2023-04-25T20:22:38.605484",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.597146",
          "status": "completed"
        },
        "tags": [],
        "id": "7fb2b357"
      },
      "source": [
        "# **Training function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc42d3ee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.624307Z",
          "iopub.status.busy": "2023-04-25T20:22:38.624016Z",
          "iopub.status.idle": "2023-04-25T20:22:38.642532Z",
          "shell.execute_reply": "2023-04-25T20:22:38.641421Z"
        },
        "papermill": {
          "duration": 0.030584,
          "end_time": "2023-04-25T20:22:38.644723",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.614139",
          "status": "completed"
        },
        "tags": [],
        "id": "fc42d3ee"
      },
      "outputs": [],
      "source": [
        "# Train loop\n",
        "\n",
        "def train_loop(folds, fold):\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "    # Loader\n",
        "    trn_idx = folds[folds['fold'] != fold].index\n",
        "    val_idx = folds[folds['fold'] == fold].index\n",
        "\n",
        "    train_folds  = folds.loc[trn_idx].reset_index(drop = True)\n",
        "    valid_folds  = folds.loc[val_idx].reset_index(drop = True)\n",
        "    valid_labels = valid_folds['InChI'].values\n",
        "\n",
        "    train_dataset = TrainDataset(train_folds, tokenizer, transform = get_transforms(data = 'train'))\n",
        "    valid_dataset = TestDataset(valid_folds, transform = get_transforms(data = 'valid'))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,batch_size = CFG.batch_size,shuffle = True,num_workers = CFG.num_workers,pin_memory = True,\n",
        "                              drop_last = True, collate_fn  = bms_collate)\n",
        "    valid_loader = DataLoader(valid_dataset,batch_size  = CFG.batch_size, shuffle = False, num_workers = CFG.num_workers,pin_memory  = True, \n",
        "                              drop_last = False)\n",
        "    \n",
        "    # Scheduler \n",
        "    def get_scheduler(optimizer):\n",
        "        if CFG.scheduler=='ReduceLROnPlateau':\n",
        "            scheduler = ReduceLROnPlateau(optimizer, \n",
        "                                          mode     = 'min', \n",
        "                                          factor   = CFG.factor, \n",
        "                                          patience = CFG.patience, \n",
        "                                          verbose  = True, \n",
        "                                          eps      = CFG.eps)\n",
        "        elif CFG.scheduler=='CosineAnnealingLR':\n",
        "            scheduler = CosineAnnealingLR(optimizer, \n",
        "                                          T_max      = CFG.T_max, \n",
        "                                          eta_min    = CFG.min_lr, \n",
        "                                          last_epoch = -1)\n",
        "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
        "                                                    T_0        = CFG.T_0, \n",
        "                                                    T_mult     = 1, \n",
        "                                                    eta_min    = CFG.min_lr, \n",
        "                                                    last_epoch = -1)\n",
        "        return scheduler\n",
        "\n",
        "    # Model & optimizer\n",
        "    encoder = Encoder(CFG.model_name, pretrained = True)\n",
        "    encoder.to(device)\n",
        "    encoder_optimizer = Adam(encoder.parameters(),lr = CFG.encoder_lr,weight_decay = CFG.weight_decay, amsgrad = False)\n",
        "    encoder_scheduler = get_scheduler(encoder_optimizer)\n",
        "    decoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n",
        "                                   embed_dim     = CFG.embed_dim, \n",
        "                                   encoder_dim   = CFG.enc_size,\n",
        "                                   decoder_dim   = CFG.decoder_dim,\n",
        "                                   num_layers    = CFG.decoder_layers,\n",
        "                                   vocab_size    = len(tokenizer), \n",
        "                                   dropout       = CFG.dropout, \n",
        "                                   device        = device)\n",
        "    decoder.to(device)\n",
        "    decoder_optimizer = Adam(decoder.parameters(), lr = CFG.decoder_lr, weight_decay = CFG.weight_decay, amsgrad = False)\n",
        "    decoder_scheduler = get_scheduler(decoder_optimizer)\n",
        "\n",
        "    # Loop\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n",
        "    best_score = np.inf\n",
        "    best_loss  = np.inf\n",
        "    for epoch in range(CFG.epochs):\n",
        "        start_time = time.time()\n",
        "        # Train\n",
        "        avg_loss = train_fn(train_loader, encoder, decoder, criterion,encoder_optimizer, decoder_optimizer, epoch,encoder_scheduler, decoder_scheduler, device)\n",
        "        # Evaluation\n",
        "        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n",
        "        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n",
        "        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n",
        "        LOGGER.info(f\"preds: {text_preds[:5]}\")\n",
        "        # Scoring\n",
        "        score = get_score(valid_labels, text_preds)\n",
        "        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n",
        "            encoder_scheduler.step(score)\n",
        "        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n",
        "            encoder_scheduler.step()\n",
        "        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n",
        "            encoder_scheduler.step()\n",
        "            \n",
        "        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n",
        "            decoder_scheduler.step(score)\n",
        "        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n",
        "            decoder_scheduler.step()\n",
        "        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n",
        "            decoder_scheduler.step()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
        "        \n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'encoder': encoder.state_dict(), \n",
        "                        'encoder_optimizer': encoder_optimizer.state_dict(), \n",
        "                        'encoder_scheduler': encoder_scheduler.state_dict(), \n",
        "                        'decoder': decoder.state_dict(), \n",
        "                        'decoder_optimizer': decoder_optimizer.state_dict(), \n",
        "                        'decoder_scheduler': decoder_scheduler.state_dict(), \n",
        "                        'text_preds': text_preds,\n",
        "                       },\n",
        "                        OUTPUT_DIR+'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8456f4ee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.663838Z",
          "iopub.status.busy": "2023-04-25T20:22:38.663098Z",
          "iopub.status.idle": "2023-04-25T20:22:38.668578Z",
          "shell.execute_reply": "2023-04-25T20:22:38.667680Z"
        },
        "papermill": {
          "duration": 0.017254,
          "end_time": "2023-04-25T20:22:38.670629",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.653375",
          "status": "completed"
        },
        "tags": [],
        "id": "8456f4ee"
      },
      "outputs": [],
      "source": [
        "def get_train_file_path(image_id):\n",
        "    return CFG.train_path + \"{}/{}/{}/{}.png\".format(image_id[0], image_id[1], image_id[2], image_id )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01d6f9f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.689062Z",
          "iopub.status.busy": "2023-04-25T20:22:38.688788Z",
          "iopub.status.idle": "2023-04-25T20:22:38.695283Z",
          "shell.execute_reply": "2023-04-25T20:22:38.694324Z"
        },
        "papermill": {
          "duration": 0.018375,
          "end_time": "2023-04-25T20:22:38.697364",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.678989",
          "status": "completed"
        },
        "tags": [],
        "id": "b01d6f9f"
      },
      "outputs": [],
      "source": [
        "# Transformations\n",
        "\n",
        "def get_transforms(*, data):\n",
        "    if data == 'train':\n",
        "        return Compose([\n",
        "            Resize(CFG.size, CFG.size),\n",
        "            HorizontalFlip(p=0.5),                  \n",
        "            Transpose(p=0.5),\n",
        "            HorizontalFlip(p=0.5),\n",
        "            VerticalFlip(p=0.5),\n",
        "            ShiftScaleRotate(p=0.5),   \n",
        "            Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    elif data == 'valid':\n",
        "        return Compose([\n",
        "            Resize(CFG.size, CFG.size),\n",
        "            Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ab18f0",
      "metadata": {
        "papermill": {
          "duration": 0.008475,
          "end_time": "2023-04-25T20:22:38.714166",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.705691",
          "status": "completed"
        },
        "tags": [],
        "id": "95ab18f0"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91251b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:38.734044Z",
          "iopub.status.busy": "2023-04-25T20:22:38.732921Z",
          "iopub.status.idle": "2023-04-25T20:22:53.004519Z",
          "shell.execute_reply": "2023-04-25T20:22:53.003401Z"
        },
        "papermill": {
          "duration": 14.284513,
          "end_time": "2023-04-25T20:22:53.007192",
          "exception": false,
          "start_time": "2023-04-25T20:22:38.722679",
          "status": "completed"
        },
        "tags": [],
        "id": "f91251b5",
        "outputId": "8e9fe4c8-7963-45c2-9549-ea4ed43cf2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>InChI</th>\n",
              "      <th>InChI_1</th>\n",
              "      <th>InChI_text</th>\n",
              "      <th>InChI_length</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000011a64c74</td>\n",
              "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
              "      <td>C13H20OS</td>\n",
              "      <td>C 13 H 20 O S /c 1 - 9 ( 2 ) 8 - 15 - 13 - 6 -...</td>\n",
              "      <td>59</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000019cc0cd2</td>\n",
              "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
              "      <td>C21H30O4</td>\n",
              "      <td>C 21 H 30 O 4 /c 1 - 12 ( 22 ) 25 - 14 - 6 - 8...</td>\n",
              "      <td>108</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000252b6d2b</td>\n",
              "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
              "      <td>C24H23N5O4</td>\n",
              "      <td>C 24 H 23 N 5 O 4 /c 1 - 14 - 13 - 15 ( 7 - 8 ...</td>\n",
              "      <td>112</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000026b49b7e</td>\n",
              "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
              "      <td>C17H24N2O4S</td>\n",
              "      <td>C 17 H 24 N 2 O 4 S /c 1 - 12 ( 20 ) 18 - 13 (...</td>\n",
              "      <td>108</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000026fc6c36</td>\n",
              "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
              "      <td>C10H19N3O2S</td>\n",
              "      <td>C 10 H 19 N 3 O 2 S /c 1 - 15 - 10 ( 14 ) 12 -...</td>\n",
              "      <td>72</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_id                                              InChI  \\\n",
              "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...   \n",
              "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...   \n",
              "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...   \n",
              "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...   \n",
              "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...   \n",
              "\n",
              "       InChI_1                                         InChI_text  \\\n",
              "0     C13H20OS  C 13 H 20 O S /c 1 - 9 ( 2 ) 8 - 15 - 13 - 6 -...   \n",
              "1     C21H30O4  C 21 H 30 O 4 /c 1 - 12 ( 22 ) 25 - 14 - 6 - 8...   \n",
              "2   C24H23N5O4  C 24 H 23 N 5 O 4 /c 1 - 14 - 13 - 15 ( 7 - 8 ...   \n",
              "3  C17H24N2O4S  C 17 H 24 N 2 O 4 S /c 1 - 12 ( 20 ) 18 - 13 (...   \n",
              "4  C10H19N3O2S  C 10 H 19 N 3 O 2 S /c 1 - 15 - 10 ( 14 ) 12 -...   \n",
              "\n",
              "   InChI_length                                          file_path  \n",
              "0            59  ../input/wowdao-generative-ai-applications-dat...  \n",
              "1           108  ../input/wowdao-generative-ai-applications-dat...  \n",
              "2           112  ../input/wowdao-generative-ai-applications-dat...  \n",
              "3           108  ../input/wowdao-generative-ai-applications-dat...  \n",
              "4            72  ../input/wowdao-generative-ai-applications-dat...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train = pd.read_pickle(CFG.prep_path + 'train.pkl')\n",
        "train['file_path'] = train['image_id'].apply(get_train_file_path)\n",
        "print(\"Training dataset:\")\n",
        "display(train.head())\n",
        "\n",
        "if CFG.debug:\n",
        "    CFG.epochs = 1\n",
        "    train = train.sample(n = CFG.samp_size, random_state = CFG.seed).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0911b22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:53.026969Z",
          "iopub.status.busy": "2023-04-25T20:22:53.026644Z",
          "iopub.status.idle": "2023-04-25T20:22:53.110591Z",
          "shell.execute_reply": "2023-04-25T20:22:53.109170Z"
        },
        "papermill": {
          "duration": 0.097183,
          "end_time": "2023-04-25T20:22:53.113830",
          "exception": false,
          "start_time": "2023-04-25T20:22:53.016647",
          "status": "completed"
        },
        "tags": [],
        "id": "a0911b22",
        "outputId": "8d09f724-5dda-4c82-cd30-ae938ece79fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folds dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>InChI</th>\n",
              "      <th>InChI_1</th>\n",
              "      <th>InChI_text</th>\n",
              "      <th>InChI_length</th>\n",
              "      <th>file_path</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8200059001fe</td>\n",
              "      <td>InChI=1S/C21H22N6O4/c1-10-19-14(27-21(22)24-10...</td>\n",
              "      <td>C21H22N6O4</td>\n",
              "      <td>C 21 H 22 N 6 O 4 /c 1 - 10 - 19 - 14 ( 27 - 2...</td>\n",
              "      <td>124</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58c5db73d7f8</td>\n",
              "      <td>InChI=1S/C28H34O4/c1-4-25(31)10-7-21-8-12-27(2...</td>\n",
              "      <td>C28H34O4</td>\n",
              "      <td>C 28 H 34 O 4 /c 1 - 4 - 25 ( 31 ) 10 - 7 - 21...</td>\n",
              "      <td>113</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00959a2eb183</td>\n",
              "      <td>InChI=1S/C21H26N2O4S/c1-27-19-12-10-17(11-13-1...</td>\n",
              "      <td>C21H26N2O4S</td>\n",
              "      <td>C 21 H 26 N 2 O 4 S /c 1 - 27 - 19 - 12 - 10 -...</td>\n",
              "      <td>111</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8cbf75bf235f</td>\n",
              "      <td>InChI=1S/C16H14FN3O/c17-13-6-7-14(15(19)10-13)...</td>\n",
              "      <td>C16H14FN3O</td>\n",
              "      <td>C 16 H 14 F N 3 O /c 17 - 13 - 6 - 7 - 14 ( 15...</td>\n",
              "      <td>69</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d7a2ebeea3e0</td>\n",
              "      <td>InChI=1S/C14H14ClN3O/c1-9-8-12(18-17-9)13(19)1...</td>\n",
              "      <td>C14H14ClN3O</td>\n",
              "      <td>C 14 H 14 Cl N 3 O /c 1 - 9 - 8 - 12 ( 18 - 17...</td>\n",
              "      <td>84</td>\n",
              "      <td>../input/wowdao-generative-ai-applications-dat...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_id                                              InChI  \\\n",
              "0  8200059001fe  InChI=1S/C21H22N6O4/c1-10-19-14(27-21(22)24-10...   \n",
              "1  58c5db73d7f8  InChI=1S/C28H34O4/c1-4-25(31)10-7-21-8-12-27(2...   \n",
              "2  00959a2eb183  InChI=1S/C21H26N2O4S/c1-27-19-12-10-17(11-13-1...   \n",
              "3  8cbf75bf235f  InChI=1S/C16H14FN3O/c17-13-6-7-14(15(19)10-13)...   \n",
              "4  d7a2ebeea3e0  InChI=1S/C14H14ClN3O/c1-9-8-12(18-17-9)13(19)1...   \n",
              "\n",
              "       InChI_1                                         InChI_text  \\\n",
              "0   C21H22N6O4  C 21 H 22 N 6 O 4 /c 1 - 10 - 19 - 14 ( 27 - 2...   \n",
              "1     C28H34O4  C 28 H 34 O 4 /c 1 - 4 - 25 ( 31 ) 10 - 7 - 21...   \n",
              "2  C21H26N2O4S  C 21 H 26 N 2 O 4 S /c 1 - 27 - 19 - 12 - 10 -...   \n",
              "3   C16H14FN3O  C 16 H 14 F N 3 O /c 17 - 13 - 6 - 7 - 14 ( 15...   \n",
              "4  C14H14ClN3O  C 14 H 14 Cl N 3 O /c 1 - 9 - 8 - 12 ( 18 - 17...   \n",
              "\n",
              "   InChI_length                                          file_path  fold  \n",
              "0           124  ../input/wowdao-generative-ai-applications-dat...     3  \n",
              "1           113  ../input/wowdao-generative-ai-applications-dat...     0  \n",
              "2           111  ../input/wowdao-generative-ai-applications-dat...     3  \n",
              "3            69  ../input/wowdao-generative-ai-applications-dat...     0  \n",
              "4            84  ../input/wowdao-generative-ai-applications-dat...     2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = TrainDataset(train, tokenizer, transform = get_transforms(data='train'))\n",
        "folds = train.copy()\n",
        "Fold = StratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n",
        "    folds.loc[val_index, 'fold'] = int(n)\n",
        "folds['fold'] = folds['fold'].astype(int)\n",
        "print(\"Folds dataset:\")\n",
        "display(folds.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63675f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-25T20:22:53.133586Z",
          "iopub.status.busy": "2023-04-25T20:22:53.133269Z",
          "iopub.status.idle": "2023-04-25T21:00:22.120881Z",
          "shell.execute_reply": "2023-04-25T21:00:22.119739Z"
        },
        "papermill": {
          "duration": 2249.000751,
          "end_time": "2023-04-25T21:00:22.124029",
          "exception": false,
          "start_time": "2023-04-25T20:22:53.123278",
          "status": "completed"
        },
        "tags": [],
        "id": "e63675f6",
        "outputId": "c7a99aa1-6e0f-46ac-80c2-0820633e613f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_ra-bcdf34b7.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/2500] Data 1.398 (1.398) Elapsed 0m 7s (remain 317m 20s) Loss: 5.2970(5.2970) Encoder Grad: 1.3135  Decoder Grad: 0.8952  \n",
            "Epoch: [1][250/2500] Data 0.001 (0.006) Elapsed 3m 24s (remain 30m 34s) Loss: 1.8493(2.4307) Encoder Grad: 0.5453  Decoder Grad: 0.5375  \n",
            "Epoch: [1][500/2500] Data 0.000 (0.003) Elapsed 6m 41s (remain 26m 42s) Loss: 1.5170(2.0252) Encoder Grad: 0.2347  Decoder Grad: 0.3790  \n",
            "Epoch: [1][750/2500] Data 0.000 (0.002) Elapsed 9m 57s (remain 23m 11s) Loss: 1.4499(1.8476) Encoder Grad: 0.3315  Decoder Grad: 0.4234  \n",
            "Epoch: [1][1000/2500] Data 0.000 (0.002) Elapsed 13m 15s (remain 19m 51s) Loss: 1.4152(1.7346) Encoder Grad: 0.2994  Decoder Grad: 0.3282  \n",
            "Epoch: [1][1250/2500] Data 0.000 (0.002) Elapsed 16m 36s (remain 16m 34s) Loss: 1.2867(1.6505) Encoder Grad: 0.3618  Decoder Grad: 0.4281  \n",
            "Epoch: [1][1500/2500] Data 0.000 (0.001) Elapsed 20m 0s (remain 13m 19s) Loss: 1.1648(1.5835) Encoder Grad: 0.2449  Decoder Grad: 0.3862  \n",
            "Epoch: [1][1750/2500] Data 0.000 (0.001) Elapsed 23m 27s (remain 10m 2s) Loss: 1.1963(1.5274) Encoder Grad: 0.3198  Decoder Grad: 0.4349  \n",
            "Epoch: [1][2000/2500] Data 0.001 (0.001) Elapsed 26m 56s (remain 6m 43s) Loss: 1.0626(1.4798) Encoder Grad: 0.2741  Decoder Grad: 0.3157  \n",
            "Epoch: [1][2250/2500] Data 0.001 (0.001) Elapsed 30m 23s (remain 3m 21s) Loss: 1.0894(1.4381) Encoder Grad: 0.3630  Decoder Grad: 0.3764  \n",
            "Epoch: [1][2499/2500] Data 0.000 (0.001) Elapsed 33m 49s (remain 0m 0s) Loss: 1.0773(1.4010) Encoder Grad: 0.2944  Decoder Grad: 0.4642  \n",
            "EVAL: [0/625] Data 0.832 (0.832) Elapsed 0m 1s (remain 13m 49s) \n",
            "EVAL: [250/625] Data 0.000 (0.004) Elapsed 1m 24s (remain 2m 6s) \n",
            "EVAL: [500/625] Data 0.001 (0.002) Elapsed 2m 48s (remain 0m 41s) \n",
            "EVAL: [624/625] Data 0.000 (0.008) Elapsed 3m 33s (remain 0m 0s) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "labels: ['InChI=1S/C28H34O4/c1-4-25(31)10-7-21-8-12-27(20(3)13-21)28-15-26(11-5-19(28)2)32-18-22-6-9-23(16-29)24(14-22)17-30/h5-6,8-9,11-15,25,29-31H,4,7,10,16-18H2,1-3H3'\n",
            " 'InChI=1S/C16H14FN3O/c17-13-6-7-14(15(19)10-13)16(21)20(9-8-18)11-12-4-2-1-3-5-12/h1-7,10H,9,11,19H2'\n",
            " 'InChI=1S/C25H23N3O2S/c1-16(2)15-28-23(21-11-6-12-31-21)22(19-9-3-4-10-20(19)25(28)30)24(29)27-18-8-5-7-17(13-18)14-26/h3-13,16,22-23H,15H2,1-2H3,(H,27,29)'\n",
            " 'InChI=1S/C14H20N2O3/c1-3-4-7-16(9-13(15)18)14(19)12-6-5-11(17)8-10(12)2/h5-6,8,17H,3-4,7,9H2,1-2H3,(H2,15,18)'\n",
            " 'InChI=1S/C12H24N2O/c1-8(2)10-5-4-6-11(7-10)14-9(3)12(13)15/h8-11,14H,4-7H2,1-3H3,(H2,13,15)']\n",
            "preds: ['InChI=1S/C30H36NO3/c1-4-5-7-21-8-10-24(13-17-27)23-9-10-23(11-12-25)23-9-10-23(2)14-20(3)27(27)27(29)27(29)27(29)29/h5-9,14-16,21,25,29,33H,4-5,11-15,20H2,1-3H3', 'InChI=1S/C15H14FN3O/c16-13-5-1-2-8-14(11)18-11-16(19)18-9-10-19(18)14-5-3-4-6-15(14)18/h1-8,12,18H,9-10H2', 'InChI=1S/C25H25N2O3S/c1-15(2)24-21(27)13-23(27)26-19-9-5-7-17(19)12-22(25)26-19-8-4-5-12-21(19)27-22(28)11-13-28-22/h3-11,14,17H,12-13H2,1-2H3,(H,25,28)', 'InChI=1S/C14H22N2O3/c1-3-4-9-17(10-10-5-7-13(17)9-9-12)14(17)10(2)14(17)18/h6-8,12,16H,3-5,9-10H2,1-2H3,(H,17,19)', 'InChI=1S/C13H25N2O/c1-9(2)11-6-8-13(11(3)11(4)12)9-12(3)15/h10-12,15H,5-9H2,1-4H3,(H,14,15)']\n",
            "Epoch 1 - avg_train_loss: 1.4010  time: 2244s\n",
            "Epoch 1 - Score: 59.6202\n",
            "Epoch 1 - Save Best Score: 59.6202 Model\n"
          ]
        }
      ],
      "source": [
        "train_loop(folds, CFG.trn_fold)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2282.178432,
      "end_time": "2023-04-25T21:00:24.961866",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-04-25T20:22:22.783434",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}